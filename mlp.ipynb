{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import load_data, prepare_data\n",
    "from config import CATEGORICAL_COLUMNS\n",
    "\n",
    "species = \"oak\"\n",
    "group_by_col = \"plot_id\"\n",
    "\n",
    "df = load_data(species)\n",
    "\n",
    "# Prepare data\n",
    "X, y = prepare_data(\n",
    "    df,\n",
    "    plotting=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode categorical columns\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def normalize(\n",
    "    data: pl.DataFrame, target: pl.DataFrame\n",
    ") -> tuple[F.Tensor, F.Tensor, list[str]]:\n",
    "    # Impute missing values with mean\n",
    "    data = data.fill_null(strategy=\"mean\")\n",
    "\n",
    "    # Convert to tensor\n",
    "    X = data.to_torch().to(torch.float32)\n",
    "    y = target.to_torch().to(torch.float32).view(-1, 1)\n",
    "\n",
    "    # Perform one-hot encoding of categorical columns\n",
    "    cols: list[str] = []\n",
    "    features: list[torch.tensor] = []\n",
    "\n",
    "    for i, col in enumerate(data.columns):\n",
    "        if col in CATEGORICAL_COLUMNS:\n",
    "            num_classes = data[col].n_unique()\n",
    "\n",
    "            features.extend([f\"{col}_{i}\" for i in range(num_classes)])\n",
    "            cols.append(F.one_hot(X[:, i].to(torch.int64), num_classes=num_classes))\n",
    "        else:\n",
    "            features.append(col)\n",
    "            cols.append(X[:, i].unsqueeze(1))\n",
    "\n",
    "    X = torch.cat(cols, dim=1)\n",
    "\n",
    "    assert X.shape[1] == len(features)\n",
    "    assert X.shape[0] == y.shape[0]\n",
    "\n",
    "    # Normalize datas\n",
    "    return (X - X.mean(axis=0)) / X.std(axis=0), (y - y.mean()) / y.std(), features\n",
    "\n",
    "\n",
    "X, y_true, features = normalize(X, y)\n",
    "X = X.nan_to_num(0.0)\n",
    "\n",
    "assert not torch.isnan(X).any(), \"Input contains NaNs\"\n",
    "assert not torch.isinf(X).any(), \"Input contains Inf\"\n",
    "assert not torch.isnan(y_true).any(), \"Target contains NaNs\"\n",
    "assert not torch.isinf(y_true).any(), \"Target contains Inf\"\n",
    "\n",
    "X.shape, y_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "train_idx, test_idx = next(gss.split(X, y_true, groups=df[group_by_col]))\n",
    "X_train, X_test = X[train_idx], X[test_idx]\n",
    "y_train, y_test = y_true[train_idx], y_true[test_idx]\n",
    "\n",
    "assert X_train.shape[0] == y_train.shape[0]\n",
    "assert X_test.shape[0] == y_test.shape[0]\n",
    "assert X_test.shape[1] == X_train.shape[1]\n",
    "assert y_test.shape[1] == y_train.shape[1]\n",
    "assert y_test.shape[1] == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simple MLP model\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_dim: int, hidden_dims: tuple[int, int, int], dropout: float = 0.0\n",
    "    ):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.dropout0 = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dims[0])\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(hidden_dims[0], hidden_dims[1])\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.fc3 = nn.Linear(hidden_dims[1], hidden_dims[2])\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        self.fc4 = nn.Linear(hidden_dims[2], 1)\n",
    "\n",
    "        # Initialize weights\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        nn.init.xavier_uniform_(self.fc3.weight)\n",
    "        nn.init.xavier_uniform_(self.fc4.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class NonLinear(nn.Module):\n",
    "    def __init__(self, input_dim: int):\n",
    "        super(NonLinear, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 8)\n",
    "        self.fc2 = nn.Linear(8, 1)\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self, input_dim: int):\n",
    "        super(Linear, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, 1)\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "model = Linear(X.shape[1])\n",
    "# model = MLP(X.shape[1], (32, 16, 8), dropout=0.2)\n",
    "print(f\"# parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "print(f\"# samples: {len(df)}\")\n",
    "print(f\"# features: {len(features)}\")\n",
    "print(f\"# train samples: {len(train_idx)}\")\n",
    "print(f\"# test samples: {len(test_idx)}\")\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 256\n",
    "num_epochs = 100\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Use Adam optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train model and keep track of losses\n",
    "train_loss_mean = []\n",
    "train_loss_std = []\n",
    "test_loss_mean = []\n",
    "test_r2_scores = []\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Create mini-batches\n",
    "    indices = torch.randperm(X_train.shape[0])\n",
    "\n",
    "    mini_batch_losses = []\n",
    "    for i in range(0, X_train.shape[0], batch_size):\n",
    "        batch_indices = indices[i : i + batch_size]\n",
    "        X_batch = X_train[batch_indices]\n",
    "        y_batch = y_train[batch_indices]\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred_test = model(X_batch)\n",
    "        train_loss = criterion(y_pred_test, y_batch)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Average loss over mini-batches\n",
    "        mini_batch_losses.append(train_loss.item())\n",
    "\n",
    "    # Compute mean and standard deviation of the train loss\n",
    "    train_loss_mean.append(np.mean(mini_batch_losses))\n",
    "    train_loss_std.append(np.std(mini_batch_losses))\n",
    "\n",
    "    # Compute loss and R2 score on the test set\n",
    "    with torch.no_grad():\n",
    "        y_pred_test = model(X_test)\n",
    "        test_loss = criterion(y_pred_test, y_test)\n",
    "        test_r2 = r2_score(y_test.numpy(), y_pred_test.numpy())\n",
    "\n",
    "        test_loss_mean.append(test_loss.item())\n",
    "        test_r2_scores.append(test_r2)\n",
    "\n",
    "        # Save model if it has the best R2 score\n",
    "        if test_r2 == max(test_r2_scores):\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(\n",
    "            f\"Epoch {epoch}\"\n",
    "            f\" - Loss: {train_loss_mean[-1]:.4f} +/- {train_loss_std[-1]:.4f}\"\n",
    "            f\" - Test Loss: {test_loss_mean[-1]:.4f}\"\n",
    "            f\" - Test R2: {test_r2_scores[-1]:.4f}\"\n",
    "        )\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the entire dataset\n",
    "with torch.no_grad():\n",
    "    y_pred_test = model(X_test)\n",
    "    y_pred_train = model(X_train)\n",
    "\n",
    "    # Calculate final loss and R2 score\n",
    "    test_loss = criterion(y_pred_test, y_test)\n",
    "    train_loss = criterion(y_pred_train, y_train)\n",
    "\n",
    "    test_r2 = r2_score(y_test.numpy(), y_pred_test.numpy())\n",
    "    train_r2 = r2_score(y_train.numpy(), y_pred_train.numpy())\n",
    "\n",
    "    print(f\"Score for species {species}\")\n",
    "    print(f\"Test loss: {test_loss.item():.4f}\")\n",
    "    print(f\"Train loss: {train_loss.item():.4f}\")\n",
    "    print(f\"R2 score (test): {test_r2:.4f}\")\n",
    "    print(f\"R2 score (train): {train_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot train losses\n",
    "plt.plot(train_loss_mean, label=\"Train Loss (mean)\")\n",
    "plt.fill_between(\n",
    "    range(len(train_loss_mean)),\n",
    "    np.array(train_loss_mean) - np.array(train_loss_std),\n",
    "    np.array(train_loss_mean) + np.array(train_loss_std),\n",
    "    alpha=0.3,\n",
    "    label=\"Train Loss (std)\",\n",
    ")\n",
    "\n",
    "# Plot test losses\n",
    "plt.plot(test_loss_mean, label=\"Test Loss\")\n",
    "\n",
    "ax1 = plt.gca()\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(test_r2_scores, color=\"green\", label=\"Test R2\")\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax2.set_ylabel(\"R2 score\")\n",
    "\n",
    "ax1.legend(loc=\"best\")\n",
    "ax2.legend(loc=\"lower right\")\n",
    "\n",
    "plt.title(\"Training Loss\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show predictions\n",
    "plt.plot(y_test, y_test, color=\"red\")\n",
    "plt.scatter(y_test, y_pred_test)\n",
    "plt.xlabel(\"True\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "\n",
    "test_r2 = r2_score(y_test.numpy(), y_pred_test.numpy())\n",
    "plt.title(f\"Predicted vs True (R2 = {test_r2:.4f})\")\n",
    "\n",
    "plt.legend([\"Predicted\", \"True\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance of Linear model\n",
    "if isinstance(model, Linear):\n",
    "    plt.figure()\n",
    "\n",
    "    plt.barh(features, model.fc.weight.squeeze().abs().detach().numpy())\n",
    "\n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.title(\"Feature importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
